{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca789b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from sqlalchemy import create_engine, MetaData, Table, select, text\n",
    "# from airflow import DAG\n",
    "from datetime import datetime, timedelta\n",
    "# from airflow.models import Variable\n",
    "# from airflow.operators.python_operator import PythonOperator\n",
    "import logging\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "# from sqlalchemy import create_engine, MetaData, Table, select, text, insert, delete, truncate\n",
    "\n",
    "# AWS 계정의 인증 정보 설정\n",
    "aws_access_key_id='***'\n",
    "aws_secret_access_key='***'\n",
    "region_name = 'ap-northeast-2'\n",
    "\n",
    "def check_foreign_key_constraints(df, engine, target_table_name):\n",
    "    metadata = MetaData(bind=engine)\n",
    "    target_table = Table(target_table_name, metadata, autoload_with=engine, autoload=True)\n",
    "    \n",
    "    # 외래키 제약조건 확인\n",
    "    foreign_keys = target_table.foreign_keys\n",
    "    if not foreign_keys:\n",
    "        return True  \n",
    "    \n",
    "    for fk in foreign_keys:\n",
    "        referenced_table = fk.column.table  # 참조되는 테이블\n",
    "        referenced_column = fk.column.name  # 참조되는 컬럼\n",
    "        \n",
    "        # 참조되는 테이블의 모든 데이터를 가져옵니다.\n",
    "        referenced_data = pd.read_sql_table(referenced_table.name, engine)\n",
    "        \n",
    "        # DataFrame의 해당 컬럼과 참조되는 테이블의 컬럼을 비교하여 외래키 제약을 확인합니다.\n",
    "        if not df[fk.parent.name].isin(referenced_data[referenced_column]).all():\n",
    "            return False  # 제약조건 위반 발견\n",
    "    \n",
    "    return True  # 모든 외래키 제약조건을 충족\n",
    "\n",
    "def list_files(bucket_name, prefix):\n",
    "    s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    files = [file['Key'] for file in response['Contents'] if file['Key'].endswith('.csv')]\n",
    "    return files\n",
    "\n",
    "def load_file_to_rds(s3_file_path, db_connection_string, table_name):\n",
    "    try:\n",
    "        # S3 파일을 pandas DataFrame으로 읽기\n",
    "        df = pd.read_csv(s3_file_path, on_bad_lines='skip')\n",
    "        \n",
    "        # SQLAlchemy 엔진 생성\n",
    "        engine = create_engine(db_connection_string)\n",
    "\n",
    "        # 외래키 제약조건 검사\n",
    "        # if not check_foreign_key_constraints(df, engine, table_name):\n",
    "        #     raise ValueError(\"외래키 제약조건 위반 데이터가 있습니다.\")\n",
    "        \n",
    "        # DataFrame을 RDS에 적재\n",
    "        df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"IntegrityError 발생: {e}. 파일: {s3_file_path} - 데이터를 적재하지 못했습니다.\")\n",
    "\n",
    "def upload_csvdata_to_rds(schema):\n",
    "\n",
    "    rds_engine = get_RDS_engine()\n",
    "    metadata_rds_con = MetaData(bind=rds_engine, schema=schema)\n",
    "    \n",
    "    bucket_name = 'de-4-1-glue-test'\n",
    "    s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)\n",
    "    \n",
    "    target_table = Table('restaurant_restaurantinfo', metadata_rds_con, autoload_with=rds_engine, autoload=True)\n",
    "    select_all_stmt = select([target_table])\n",
    "    existing_data = rds_engine.execute(select_all_stmt)\n",
    "\n",
    "bucket_name = 'de-4-1-glue-test'\n",
    "prefix = 'backend/restaurant_restaurantinfo'\n",
    "db_connection_string = 'mysql://admin:de-4-1-mysql@de-4-1-mysql.***.ap-northeast-2.rds.amazonaws.com:3306/de41mysql'\n",
    "table_name = 'restaurant_restaurantinfo'\n",
    "\n",
    "files = list_files(bucket_name, prefix)\n",
    "\n",
    "for file in files:\n",
    "    s3_file_path = f\"s3://{bucket_name}/{file}\"\n",
    "    load_file_to_rds(s3_file_path, db_connection_string, table_name)\n",
    "    print(f\"{file} has been loaded to RDS.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
